---
layout:     post                 # 使用的布局（不需要改）
title:      Seq2Seq               # 标题 
subtitle:   Neural Sequence-toSequence Models #副标题
date:       2019-02-19              # 时间
author:     Yichen Yang                      # 作者
header-img: img/post-bg-halting.jpg  #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - DL
    - Paper
---
# 基于神经网络的序列到序列架构
> 讲述Seq2Seq的原理和算法，文献可在这里[下载](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf),
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
## 摘要
深度神经网络（DNN）是功能强大的模型，在艰难的学习任务中取得了卓越的性能。虽然DNN在大型标记训练集可用时运行良好，但它们不能用于将序列映射到序列。在本文中，我们提出了一种通用的端到端序列学习方法，它对序列结构做出了最小的假设。**我们的方法使用多层长短期记忆`（LSTM）`将输入序列映射到固定维度的矢量***(这里根据注意力机制得以改进)***，然后使用另一个深LSTM来解码来自矢量的目标序列。**我们的主要结果是，在WMT-14数据集的英语到法语翻译任务中，LSTM产生的翻译在整个测试集上获得了34.8的BLEU评分，其中LSTM的BLEU评分在词汇表外被惩罚话。此外，LSTM对长句也没有困难。为了比较，基于短语的SMT系统在同一数据集上实现了33.3的BLEU分数。当我们使用LSTM重新调整上述SMT系统产生的1000个假设时，其BLEU分数增加到36.5，这接近于先前的技术水平。 LSTM还学习了对词序有敏感且对主动和被动语音相对不变的合理的短语和句子表示。最后，我们发现在所有源语句（但不是目标句子）中反转单词的顺序显着改善了LSTM的表现，因为这样做引入了源语句和目标语句之间的许多短期依赖关系，这使得优化问题更容易。

## 介绍
尽管DNN具有灵活性和功能，**但它们只能应用于输入和目标可以用固定维数向量进行合理编码的问题。这是一个重要的限制，因为许多重要问题最好用长度未知的序列表达。** 例如，语音识别和机器翻译是顺序问题。同样，问题回答也可以被视为将表示问题的单词序列映射到表示答案的单词序列。因此很清楚，**学习将序列映射到序列的与结构域无关的方法将是有用的**。

序列对DNN构成挑战，因为它们要求输入和输出的维度已知且固定。在本文中，我们展示了长短期记忆（LSTM）架构的直接应用可以解决序列问题的一般序列。我们的想法是使用一个LSTM来读取输入序列，一次一个步骤，以获得大的固定维向量表示，然后使用另一个LSTM从该向量中提取输出序列（图1）。第二个LSTM本质上是一个递归神经网络语言模型，除了它以输入序列为条件。 **LSTM成功学习具有长距离时间依赖性的数据的能力使其成为该应用的自然选择**，因为输入与其相应输出之间存在相当大的时间滞后（图1）。
![](https://ws1.sinaimg.cn/large/006tKfTcgy1g0d8vf5n2dj311m098wfc.jpg)

已经有许多相关的尝试来解决用神经网络对序列学习问题的一般序列。我们的方法与Kalchbrenner和Blunsom密切相关，他们是第一个将整个输入句子映射到向量的方法，与[Cho](https://arxiv.org/pdf/1409.0473.pdf)等人非常相似。 **Graves引入了一种新颖的可区分注意机制**,允许神经网络专注于他们输入的不同部分，并且这个想法的优雅变体成功地应用于[Bahdanau](https://arxiv.org/pdf/1409.0473.pdf)等人的机器翻译。连接主义序列分类是另一种用于将序列映射到具有神经网络的序列的流行技术，尽管它假设输入和输出之间存在单调对齐。

令人惊讶的是，尽管最近有其他相关架构研究人员的经验，LSTM并没有受到很长时间的影响。我们能够在长句子上做得好，因为我们颠倒了源句中的单词顺序，而不是训练和测试集中的目标句子。**通过这样做，我们引入了许多短期依赖关系，使优化问题更加简单（参见第2节和第3.3节）？？？**。因此，SGD可以学习长句无误的LSTM。扭转源句中单词的简单技巧是这项工作的关键技术贡献之一。

LSTM的一个有用特性是它学习将可变长度的输入句子映射到固定维向量表示。鉴于翻译倾向于是源句的释义，翻译目标鼓励LSTM找到捕捉其含义的句子表示，因为具有相似含义的句子彼此接近，而不同的句子含义将是远的。定性评估支持这种说法，表明我们的模型知道单词顺序，并且对主动和被动语音是相当不变的。

## 模型
递归神经网络(RNN)是前馈神经网络与序列的自然概括，示意图如下所示。给定一系列输入 $$(x_1，...，x_T)$$，标准RNN通过迭代以下等式计算输出序列 $$(y_1，...，y_T)$$:

$$
h_t = sigm(W^{hx}x_t + W^{hh}h_{t-1})
$$

$$
y_t = W^{yh}h_t
$$

只要提前知道输出之间的对齐，RNN就可以轻松地将序列映射到序列。然而，<span style="color:blue;">**目前尚不清楚如何将RNN应用于其输入和输出序列具有不同长度且具有复杂和非单调关系的问题**</span>，<span style="color:red;">**需要解决的问题**</span>。
![](https://ws3.sinaimg.cn/large/006tKfTcgy1g0e9p1lfamj30jr06l751.jpg)
一般序列学习的简单策略是使用一个RNN将输入序列映射到固定大小的载体，然后用另一个RNN将载体映射到目标序列（Cho等人也采用了这种方法）。虽然它可以原则上工作，因为RNN被提供了所有相关信息，但由于产生的长期依赖性，很难训练RNN（图1）。然而，<span style="color:blue;">**已知长短期记忆（LSTM）会学习长距离时间依赖性的问题，因此LSTM可以成功的应用到这一问题上**</span>。


LSTM的目标是估计条件概率 
$$p(y_1，...，y'_T|x_1，...，x_T)$$
，其中, $$(y_1，...，y'_T)$$和$$(x_1，...，x_T)$$分别表示输入和输出，且长度不同$$T'!= T$$。 LSTM通过首先获得由LSTM的最后隐藏状态给出的输入序列$$(x_1，...，x_T)$$的固定维度表示v，然后计算$$(y_1，...，y'_T)$$的条件概率采用标准的LSTM-LM公式，其初始隐藏状态设置为$$(x_1，...，x_T)$$的固定维度v:

$$
p(y_1，...，y'_T|x_1，...，x_T）= \prod_{t=1}^{T'} p(y_t|v，y_1，...，y_{t-1}）（1）
$$


在该等式中，每个 
$$p(y_t | v，y_1，...，y_{t-1})$$
分布用词汇表中的所有单词的`softmax`表示。我们使用`Graves的LSTM方法`。请注意，我们要求每个句子以特殊的句末符号“<EOS>”结束，这使得模型能够定义所有可能长度的序列的分布。总体方案如图1所示，其中显示的LSTM计算“A”，“B”，“C”，“EOS”的表示，然后使用该表示来计算“W”，“X”，"Y"，"Z"，"EOS"的概率。

<span style="color:blue;">**我们的实际模型在三个重要方面与上述描述不同。**</span>
* 使用了两个不同的LSTM：一个用于输入序列，另一个用于输出序列，因为这样做可以忽略的计算成本增加模型参数的数量，并且可以自然地同时在多个语言对上训练LSTM。
* 发现深LSTM明显优于浅LSTM，因此我们选择了具有四层的LSTM。
* 发现扭转输入句子的单词顺序是非常有价值的。
  <span style="color:red;">**例如，不是将句子a，b，c映射到句子α，β，γ，而是要求LSTM将c，b，a映射到α，β，γ，其中α，β，γ是平移a，b，c。这样，a非常靠近α，b非常接近β，依此类推，这使得SGD很容易在输入和输出之间“建立通信”。我们发现这种简单的数据转换可以极大地提高LSTM的性能。**</span><span style="color:green;">**???**</span>

目标函数：

$$ \max 1/|S|\sum_{(T,S)\in S} log_p(T|S)$$

其中S是训练数据集，T是预测正确的结果，所以我们的目标是最大化数据集中预测正确的概率。当产生结果后，我们应用`LSTM`寻找最有可能翻译正确的结果：

$$
\overline{T} = \arg \max_T \ p(T|S)
$$

## 算法

## 实施
1.导入必须的python包和选择设备 

```python
from __future__ import unicode_literals, print_function, division
#在老版本的Python中兼顾新特性的一种方法
from io import open
import unicodedata
import string
import re
import random

import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#检验是否可用GPU，不行选用CPU设备
```
* `from __future__ import **`
  * Python提供了`__future__`模块，为了在老版本的Python中兼顾新特性的一种方法
  * 例如：在开头加上`from __future__ import print_function`，在python2.7下使用print可以像python3.6那样加括号。
  * `division`： 整数除法； `unicode_literals`： 字符串文本成为Unicode字符串
* `torch.device()`
  * 在PyTorch中使用GPU是非常简单的，你可以将模型放到一块GPU上：
  ```python
  device = torch.device("cuda:0")
  model.to(device)
  ```
  * 然后，你可以将所有的tensors复制到GPU上(返回的是一个备份而不是重写原来的tensors)：
  ```python
  mytensor = my_tensor.to(device)
  ```

2.数据预处理

```python
import 
```

